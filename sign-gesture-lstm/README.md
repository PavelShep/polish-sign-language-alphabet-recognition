# (PL) sign-gesture-lstm

Skrypty dla trenowania modelu do rozpoznawania gest贸w doni oraz ich wykrywanie w czasie rzeczywistym z u偶yciem kamery internetowej.
Model oparty jest na sieci neuronowej LSTM i wykorzystuje punkty kluczowe doni (hand landmarks).

---

## Wymagania

- Python **3.10.11**
- Kamera internetowa
- System Windows (instrukcje poni偶ej dla Windows)

---

## Instalacja i uruchomienie

### 1锔 Jeli tworzysz rodowisko od zera

Utw贸rz wirtualne rodowisko:

```bash
python -m venv venv
```

Aktywuj rodowisko:

```bash
venv\Scripts\activate
```

Zainstaluj wymagane biblioteki:

```bash
pip install -r requirements.txt
```

### 2锔 Jeli rodowisko ju偶 istnieje

Aktywuj je:

```bash
venv\Scripts\activate
```

---

## Proces trenowania modelu

###  1. Zbieranie danych

Uruchom:

```bash
python collect_data.py
```

Skrypt zapisuje sekwencje punkt贸w doni do folderu `data/`.

###  2. Trenowanie modelu

```bash
python train_model.py
```

Po zakoczeniu treningu zostanie wygenerowany plik modelu (np. `model.h5`).

###  3. Rozpoznawanie gest贸w w czasie rzeczywistym

```bash
python detect_live.py
```

Program uruchamia kamer i wywietla przewidywan liter alfabetu PJM.

###  4. Ewaluacja modelu (rednia skuteczno)

Aby sprawdzi dokadno modelu:

```bash
python evaluate_model.py
```

Skrypt wywietli:

- dokadno (accuracy)
- macierz pomyek (confusion matrix)
- inne metryki klasyfikacji

---

## Struktura

```
 data/                # Zebrane dane (sekwencje punkt贸w doni)
 model.h5             # Wytrenowany model
 collect_data.py      # Zbieranie danych
 train_model.py       # Trenowanie modelu
 detect_live.py       # Rozpoznawanie w czasie rzeczywistym
 evaluate_model.py    # Ewaluacja modelu
 requirements.txt     # Lista zale偶noci
 README.md
```

---

## Informacje dodatkowe

- Model wykorzystuje sekwencje punkt贸w kluczowych doni (21 landmark贸w).
- Sie LSTM analizuje zale偶noci czasowe w ruchu doni.
- Skuteczno modelu zale偶y od jakoci i liczby zebranych danych.

---

![Recording preparation window interface](photo_trening.png)

---

# (ENG) sign-gesture-lstm

Scripts for training a hand gesture recognition model and real-time detection using a webcam.
The model is based on an LSTM neural network and uses hand landmarks as input features.

---

## Requirements

- Python **3.10.11**
- Webcam
- Windows OS (instructions below for Windows)

---

## Installation and Setup

### 1锔 If you are creating the environment from scratch

Create a virtual environment:

```bash
python -m venv venv
```

Activate the environment:

```bash
venv\Scripts\activate
```

Install the required libraries:

```bash
pip install -r requirements.txt
```

### 2锔 If the environment already exists

Activate it:

```bash
venv\Scripts\activate
```

---

## Model Training Process

###  1. Data Collection

Run:

```bash
python collect_data.py
```

The script saves hand landmark sequences to the `data/` folder.

###  2. Model Training

```bash
python train_model.py
```

After training is complete, a model file will be generated (e.g. `model.h5`).

###  3. Real-Time Gesture Recognition

```bash
python detect_live.py
```

The program launches the camera and displays the predicted letter of the PJM alphabet.

###  4. Model Evaluation (average accuracy)

To check model accuracy:

```bash
python evaluate_model.py
```

The script will display:

- accuracy
- confusion matrix
- other classification metrics

---

## Project Structure

```
 data/                # Collected data (hand landmark sequences)
 model.h5             # Trained model
 collect_data.py      # Data collection
 train_model.py       # Model training
 detect_live.py       # Real-time recognition
 evaluate_model.py    # Model evaluation
 requirements.txt     # List of dependencies
 README.md
```

---

## Additional Information

- The model uses hand landmark sequences (21 landmarks).
- The LSTM network analyzes temporal dependencies in hand movement.
- Model accuracy depends on the quality and quantity of collected data.
